{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_lunar_dir = \"./data/lunar/data/training/catalogs/apollo12_catalog_GradeA_final.csv\"\n",
    "data_dir = \"./data/lunar/data/training/data/S12_GradeA/Filtered\"\n",
    "chunk_size = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter Raw Test File Inof:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = \"./data/lunar/data/test/data/Filt_S16_GradeA\"\n",
    "file_name = \"xa.s16.00.mhz.1977-06-02HR00_evid00255.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process single lunar data\n",
    "\n",
    "def process_lunar_data(catalog_path, data_dir, row_num, chunk_size):\n",
    "    # Read the catalog file\n",
    "    catalog_lunar = pd.read_csv(catalog_path)\n",
    "\n",
    "    # Extract row and relevant data\n",
    "    row = catalog_lunar.iloc[row_num]\n",
    "    arrival_time = datetime.strptime(row[\"time_abs(%Y-%m-%dT%H:%M:%S.%f)\"], '%Y-%m-%dT%H:%M:%S.%f')\n",
    "    arrival_time_relative = row[\"time_rel(sec)\"]\n",
    "    test_filename = row.filename\n",
    "\n",
    "    # Read the associated CSV file containing raw data\n",
    "    csv_file = f'{data_dir}/{test_filename}.csv'\n",
    "    raw_data = pd.read_csv(csv_file)\n",
    "\n",
    "    raw_data = raw_data[raw_data['cluster_ids'] != 0]\n",
    "\n",
    "    results_df = pd.DataFrame(columns=[\"chunk\", \"label\"])\n",
    "    \n",
    "    for cluster_id in raw_data['cluster_ids'].unique():\n",
    "        # Get the data corresponding to the current cluster\n",
    "        cluster_data = raw_data[raw_data['cluster_ids'] == cluster_id]\n",
    "        \n",
    "        # Determine the start and end time of the cluster (in relative time)\n",
    "        cluster_start = cluster_data['time_rel(sec)'].min()\n",
    "        cluster_end = cluster_data['time_rel(sec)'].max()\n",
    "\n",
    "        if arrival_time_relative >= cluster_start and arrival_time_relative <= cluster_end:\n",
    "            results_df = pd.concat([results_df, pd.DataFrame({\"chunk\": [cluster_data], \"label\": [1]})], ignore_index=True)\n",
    "        else:\n",
    "            results_df = pd.concat([results_df, pd.DataFrame({\"chunk\": [cluster_data], \"label\": [0]})], ignore_index=True)\n",
    "        # # Initialize the cluster label to 0 (non-quake)\n",
    "        # raw_data.loc[raw_data['cluster_id'] == cluster_id, 'quake_cluster'] = 0\n",
    "\n",
    "        # # Loop through matching events to check if they fall within the cluster's time range\n",
    "        # for _, event_row in matching_events.iterrows():\n",
    "        #     event_time = event_row['time_rel']\n",
    "            \n",
    "        #     if is_time_in_cluster(cluster_start, cluster_end, event_time):\n",
    "        #         # If the event time falls within the cluster range, mark the cluster as a quake\n",
    "        #         raw_data.loc[raw_data['cluster_id'] == cluster_id, 'quake_cluster'] = 1\n",
    "\n",
    "    # Initialize the results dataframe\n",
    "    # total_rows = raw_data.shape[0]\n",
    "    # results_df = pd.DataFrame(columns=[\"chunk\", \"label\"])\n",
    "\n",
    "\n",
    "\n",
    "    # Iterate over chunks of data\n",
    "    # start = 0\n",
    "    # while start < total_rows:\n",
    "    #     end = min(start + chunk_size, total_rows)  # Handle case where we don't have a full chunk at the end\n",
    "    #     chunk = raw_data.iloc[start:end]\n",
    "    #     data_df = pd.DataFrame(chunk[\"time_rel(sec)\"])\n",
    "\n",
    "    #     # Check if arrival time is within the current chunk\n",
    "    #     if arrival_time_relative >= data_df[\"time_rel(sec)\"].values.min() and arrival_time_relative <= data_df[\"time_rel(sec)\"].values.max():\n",
    "    #         # Split chunk at the arrival_time_relative\n",
    "    #         before_arrival = chunk[data_df[\"time_rel(sec)\"] < arrival_time_relative]\n",
    "    #         after_arrival = chunk[data_df[\"time_rel(sec)\"] >= arrival_time_relative]\n",
    "\n",
    "    #         # Add the part before arrival_time_relative with label 0\n",
    "    #         if not before_arrival.empty:\n",
    "    #             results_df = pd.concat([results_df, pd.DataFrame({\"chunk\": [before_arrival], \"label\": [0]})], ignore_index=True)\n",
    "\n",
    "    #         # Add a chunk starting from the arrival_time_relative, ensure it has chunk_size rows\n",
    "    #         after_start = after_arrival.index[0]  # Start from the first row after the arrival time\n",
    "    #         after_end = min(after_start + chunk_size, total_rows)  # Ensure the chunk has exactly chunk_size rows\n",
    "    #         after_chunk = raw_data.iloc[after_start:after_end]\n",
    "\n",
    "    #         results_df = pd.concat([results_df, pd.DataFrame({\"chunk\": [after_chunk], \"label\": [1]})], ignore_index=True)\n",
    "\n",
    "    #         # Move the start index beyond this chunk (chunk_size after the arrival time)\n",
    "    #         start = after_end\n",
    "    #     else:\n",
    "    #         # If no arrival time in this chunk, label the entire chunk as 0\n",
    "    #         results_df = pd.concat([results_df, pd.DataFrame({\"chunk\": [chunk], \"label\": [0]})], ignore_index=True)\n",
    "    #         start += chunk_size  # Move to the next chunk\n",
    "\n",
    "    # # If there are remaining rows less than the chunk size, add them with the appropriate label\n",
    "    # if start < total_rows:\n",
    "    #     remaining_chunk = raw_data.iloc[start:total_rows]\n",
    "    #     remaining_label = 1 if arrival_time_relative >= remaining_chunk[\"time_rel(sec)\"].values.min() else 0\n",
    "    #     results_df = pd.concat([results_df, pd.DataFrame({\"chunk\": [remaining_chunk], \"label\": [remaining_label]})], ignore_index=True)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Process entire catalog\n",
    "\n",
    "def process_entire_catalog(catalog_dir, data_dir, chunk_size):\n",
    "    try:\n",
    "        catalog_lunar = pd.read_csv(catalog_dir)\n",
    "        testing_df = pd.DataFrame(columns=[\"chunk\", \"label\"])  # Initialize an empty dataframe to store results\n",
    "        \n",
    "        # Iterate over every row in the catalog\n",
    "        for row_num in range(len(catalog_lunar)):\n",
    "            # Process each row using process_lunar_data and append the result to testing_df\n",
    "            results_df = process_lunar_data(catalog_dir, data_dir, row_num, chunk_size)\n",
    "            testing_df = pd.concat([testing_df, results_df], ignore_index=True)\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found\")\n",
    "    \n",
    "    return testing_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# extracting the features\n",
    "\n",
    "def extract_features_from_chunk(chunk):\n",
    "    features = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Velocity features\n",
    "    # features['mean_velocity'] = chunk[\"mov_avg_clamped\"].mean()\n",
    "    \n",
    "    # features['max_velocity'] = chunk[\"mov_avg_clamped\"].max()\n",
    "    # features['min_velocity'] = chunk[\"mov_avg_clamped\"].min()\n",
    "    # features['std_velocity'] = chunk[\"mov_avg_clamped\"].std()\n",
    "    # features['range_velocity'] = features['max_velocity'] - features['min_velocity']\n",
    "    \n",
    "    # # Energy features\n",
    "    # features['total_energy'] = (chunk[\"mov_avg_clamped\"] ** 2).sum()\n",
    "     # # Frequency/Oscillation features (zero crossings)\n",
    "    signs = np.sign(chunk[\"mov_avg_clamped\"])\n",
    "    signs[signs == 0] = -1  # Treat zeros as negative to prevent false crossings\n",
    "\n",
    "    # Find zero crossings\n",
    "    zero_crossings = np.where(np.diff(signs))[0]\n",
    "\n",
    "    # Calculate zero-crossing rate\n",
    "    features['zero_crossing_rate'] = len(zero_crossings) / len(chunk)\n",
    "\n",
    "    # rms_velocity = np.sqrt(np.mean(chunk[\"mov_avg_clamped\"]**2))\n",
    "    # features[\"rms_velocity\"] = rms_velocity\n",
    "    \n",
    "\n",
    "\n",
    "    # mean_abs_velocity = np.mean(np.abs(chunk[\"mov_avg_clamped\"]))\n",
    "    # features[\"mean_abs_velocity\"] = mean_abs_velocity\n",
    "    \n",
    "    # total_energy = (chunk['mov_avg_clamped'] ** 2).sum()\n",
    "    # features[\"total_energy\"] = total_energy\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# converting to suitable\n",
    "\n",
    "def convert_to_polished_df(testing_df):\n",
    "    polished_data = []\n",
    "\n",
    "    for idx, row in testing_df.iterrows():\n",
    "        chunk = row[\"chunk\"]\n",
    "        label = row[\"label\"]\n",
    "        \n",
    "        # Extract statistical features from chunk\n",
    "        features = extract_features_from_chunk(chunk)\n",
    "        features['label'] = label  # Add the label for quake/no-quake\n",
    "        \n",
    "        # Append to polished data\n",
    "        polished_data.append(features)\n",
    "\n",
    "    # Convert to a new DataFrame\n",
    "    polished_df = pd.DataFrame(polished_data)\n",
    "    return polished_df\n",
    "\n",
    "\n",
    "\n",
    "def process_test_file_for_model(file_name, chunk_size):\n",
    "    # Path to the CSV file\n",
    "    csv_file = f\"{file_name}\"\n",
    "    \n",
    "    # Read the raw data from the CSV file\n",
    "    raw_data = pd.read_csv(csv_file)\n",
    "    total_rows = raw_data.shape[0]\n",
    "\n",
    "    # Lists to store statistics for each chunk and the chunks themselves\n",
    "    stats_list = []\n",
    "    chunks_list = []  # List to store the chunks\n",
    "\n",
    "    # Iterate over chunks of data in the file\n",
    "    start = 0\n",
    "    while start < total_rows:\n",
    "        end = min(start + chunk_size, total_rows)\n",
    "        chunk = raw_data.iloc[start:end]\n",
    "\n",
    "        # Store the current chunk in the chunks list\n",
    "        chunks_list.append(chunk)\n",
    "\n",
    "        # Calculate statistical values from the chunk\n",
    "        # mean_velocity = chunk['velocity(m/s)'].mean()\n",
    "        \n",
    "        # max_velocity = chunk['velocity(m/s)'].max()\n",
    "        # min_velocity = chunk['velocity(m/s)'].min()\n",
    "        # std_velocity = chunk['velocity(m/s)'].std()\n",
    "        # range_velocity = max_velocity - min_velocity\n",
    "        # total_energy = (chunk['velocity(m/s)'] ** 2).sum()\n",
    "        # rms_velocity = (chunk['velocity(m/s)'] ** 2).mean() ** 0.5\n",
    "\n",
    "        # Calculate zero crossing rate\n",
    "        zero_crossings = np.where(np.diff(np.sign(chunk[\"velocity(m/s)\"])))[0]\n",
    "        zero_crossing_rate = len(zero_crossings)  # Count of zero crossings\n",
    "\n",
    "\n",
    "        # Add the calculated statistics to the stats list\n",
    "        stats_list.append({\n",
    "            # 'mean_velocity': mean_velocity,\n",
    "            \n",
    "            # 'max_velocity': max_velocity,\n",
    "            # 'min_velocity': min_velocity,\n",
    "            # 'std_velocity': std_velocity,\n",
    "            # 'range_velocity': range_velocity,\n",
    "            # 'total_energy': total_energy,\n",
    "            # 'rms_velocity': rms_velocity,\n",
    "            'zero_crossing_rate': zero_crossing_rate\n",
    "        })\n",
    "\n",
    "        # Move to the next chunk\n",
    "        start += chunk_size\n",
    "\n",
    "    # Create DataFrames from the lists\n",
    "    stats_df = pd.DataFrame(stats_list)  # DataFrame for statistics\n",
    "    chunks_df = pd.DataFrame({'chunks': chunks_list})  # DataFrame for chunks\n",
    "\n",
    "    return stats_df, chunks_df\n",
    "\n",
    "# Process raw input into ready for model to read\n",
    "def process_raw_file_for_model(raw_data_dir, file_name, chunk_size):\n",
    "    # Path to the CSV file\n",
    "    csv_file = f\"{raw_data_dir}/{file_name}\"\n",
    "    \n",
    "    # Read the raw data from the CSV file\n",
    "    raw_data = pd.read_csv(csv_file)\n",
    "    total_rows = raw_data.shape[0]\n",
    "\n",
    "    # Lists to store statistics for each chunk and the chunks themselves\n",
    "    stats_list = []\n",
    "    chunks_list = []  # List to store the chunks\n",
    "\n",
    "    # Iterate over chunks of data in the file\n",
    "    start = 0\n",
    "    while start < total_rows:\n",
    "        end = min(start + chunk_size, total_rows)\n",
    "        chunk = raw_data.iloc[start:end]\n",
    "\n",
    "        # Store the current chunk in the chunks list\n",
    "        chunks_list.append(chunk)\n",
    "\n",
    "        # Calculate statistical values from the chunk\n",
    "        # mean_velocity = chunk['velocity(m/s)'].mean()\n",
    "        \n",
    "        # max_velocity = chunk['velocity(m/s)'].max()\n",
    "        # min_velocity = chunk['velocity(m/s)'].min()\n",
    "        # std_velocity = chunk['velocity(m/s)'].std()\n",
    "        # range_velocity = max_velocity - min_velocity\n",
    "        # total_energy = (chunk['velocity(m/s)'] ** 2).sum()\n",
    "        # rms_velocity = (chunk['velocity(m/s)'] ** 2).mean() ** 0.5\n",
    "        \n",
    "        mean_abs_velocity = np.mean(np.abs(chunk['velocity(m/s)']))\n",
    "        rms_velocity = np.sqrt(np.mean(chunk['velocity(m/s)'] ** 2))\n",
    "        # Calculate zero crossing rate\n",
    "        signs = np.sign(chunk[\"velocity(m/s)\"])\n",
    "\n",
    "        # Treat zeros as negative to avoid false zero crossings\n",
    "        signs[signs == 0] = -1\n",
    "\n",
    "        # Find zero crossings (where the sign changes)\n",
    "        zero_crossings = np.where(np.diff(signs))[0]\n",
    "\n",
    "        # Calculate the zero-crossing rate (normalized by the chunk length)\n",
    "        zero_crossing_rate = len(zero_crossings) / len(chunk[\"velocity(m/s)\"])  # Count of zero crossings\n",
    "\n",
    "        total_energy = (chunk['velocity(m/s)'] ** 2).sum()\n",
    "\n",
    "\n",
    "        # Add the calculated statistics to the stats list\n",
    "        stats_list.append({\n",
    "            # 'mean_velocity': mean_velocity,\n",
    "            \n",
    "            # 'max_velocity': max_velocity,\n",
    "            # 'min_velocity': min_velocity,\n",
    "            # 'std_velocity': std_velocity,\n",
    "            # 'range_velocity': range_velocity,\n",
    "            # 'total_energy': total_energy,\n",
    "            # 'rms_velocity': rms_velocity,\n",
    "            'zero_crossing_rate': zero_crossing_rate,\n",
    "            \"rms_velocity\": rms_velocity,\n",
    "            \"mean_abs_velocity\": mean_abs_velocity,\n",
    "            \"total_energy\": total_energy\n",
    "        })\n",
    "\n",
    "        # Move to the next chunk\n",
    "        start += chunk_size\n",
    "\n",
    "    # Create DataFrames from the lists\n",
    "    stats_df = pd.DataFrame(stats_list)  # DataFrame for statistics\n",
    "    chunks_df = pd.DataFrame({'chunks': chunks_list})  # DataFrame for chunks\n",
    "\n",
    "    return stats_df, chunks_df\n",
    "\n",
    "\n",
    "# Plot Input Data\n",
    "\n",
    "def plot_data(data_dir, file_name):\n",
    "    \n",
    "    test_filename = file_name\n",
    "    csv_file = f'{data_dir}/{test_filename}'\n",
    "    data = pd.read_csv(csv_file)\n",
    "\n",
    "    csv_times = np.array(data[\"time_rel(sec)\"].tolist())\n",
    "    csv_velocity = np.array(data[\"velocity(m/s)\"].tolist())\n",
    "\n",
    "    # Plot the trace!\n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,3))\n",
    "    ax.plot(csv_times,csv_velocity)\n",
    "    # Make the plot pretty\n",
    "    ax.set_xlim([min(csv_times),max(csv_times)])\n",
    "    ax.set_ylabel('Velocity (m/s)')\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_title(f'{test_filename}', fontweight='bold')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found\n",
      "label\n",
      "0    20\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_chunks = process_entire_catalog(catalog_lunar_dir, data_dir, chunk_size)\n",
    "# polished_df = convert_to_polished_df(testing_chunks)\n",
    "\n",
    "label_counts = testing_chunks[\"label\"].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "count_label_1 = label_counts.get(1, 0)\n",
    "\n",
    "count_label_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                chunk label\n",
       "0   Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
       "1   Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
       "2   Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
       "3   Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
       "4   Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
       "5   Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
       "6   Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
       "7   Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
       "8   Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
       "9   Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
       "10  Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
       "11  Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
       "12  Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
       "13  Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
       "14  Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
       "15  Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
       "16  Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
       "17  Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
       "18  Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
       "19  Empty DataFrame\n",
       "Columns: [time_abs(%Y-%m-%dT%H...     0"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_chunks[testing_chunks[\"label\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 1 - Using LR\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # Split the dataset into features (X) and target labels (y)\n",
    "# X = testing_chunks.drop(columns=['label'])  # Features\n",
    "# y = testing_chunks['label']  # Target labels (1 for quake, 0 for non-quake)\n",
    "\n",
    "\n",
    "# # Split data into training and testing sets (80% training, 20% testing)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# # Initialize the Random Forest Classifier\n",
    "# clf = LogisticRegression( max_iter=1000, random_state=42)\n",
    "\n",
    "# # Train the model\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2 - Balancing the data out more\n",
    "\n",
    "# df = testing_chunks\n",
    "\n",
    "# # Split the DataFrame into two separate DataFrames for each class\n",
    "# df_majority = df[df['label'] == 0]\n",
    "# df_minority = df[df['label'] == 1]\n",
    "\n",
    "# # Specify the number of majority class instances you want to keep\n",
    "# # Here we keep the same number as minority class instances\n",
    "# n_minority = len(df_minority)\n",
    "# df_majority_undersampled = df_majority.sample(n=n_minority, random_state=42)  # Randomly sample from majority class\n",
    "\n",
    "# # Combine the undersampled majority class with the minority class\n",
    "# df_balanced = pd.concat([df_majority_undersampled, df_minority])\n",
    "\n",
    "# # Shuffle the DataFrame to mix class labels\n",
    "# df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # Now `df_balanced` is your new training dataset with a balanced class distribution\n",
    "\n",
    "# # Split the dataset into features (X) and target labels (y)\n",
    "# X = df_balanced.drop(columns=['label'])  # Features\n",
    "# y = df_balanced['label']  # Target labels (1 for quake, 0 for non-quake)\n",
    "\n",
    "\n",
    "# # Split data into training and testing sets (80% training, 20% testing)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Initialize the Random Forest Classifier\n",
    "# clf2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Train the model\n",
    "# clf2.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = clf2.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                chunk label\n",
      "0   Empty DataFrame\n",
      "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
      "1   Empty DataFrame\n",
      "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
      "2   Empty DataFrame\n",
      "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
      "3   Empty DataFrame\n",
      "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
      "4   Empty DataFrame\n",
      "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
      "5   Empty DataFrame\n",
      "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
      "6   Empty DataFrame\n",
      "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
      "7   Empty DataFrame\n",
      "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
      "8   Empty DataFrame\n",
      "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
      "9   Empty DataFrame\n",
      "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
      "10  Empty DataFrame\n",
      "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
      "11  Empty DataFrame\n",
      "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
      "12  Empty DataFrame\n",
      "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
      "13  Empty DataFrame\n",
      "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
      "14  Empty DataFrame\n",
      "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
      "15  Empty DataFrame\n",
      "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
      "16  Empty DataFrame\n",
      "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
      "17  Empty DataFrame\n",
      "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
      "18  Empty DataFrame\n",
      "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
      "19  Empty DataFrame\n",
      "Columns: [time_abs(%Y-%m-%dT%H...     0\n",
      "./data/lunar/data/test/data/Filt_S16_GradeA\\xa.s16.00.mhz.1972-09-10HR00_evid00075.csv\n",
      "./data/lunar/data/test/data/Filt_S16_GradeA\\xa.s16.00.mhz.1972-11-06HR00_evid00079.csv\n",
      "./data/lunar/data/test/data/Filt_S16_GradeA\\xa.s16.00.mhz.1972-11-08HR00_evid00080.csv\n",
      "./data/lunar/data/test/data/Filt_S16_GradeA\\xa.s16.00.mhz.1972-11-14HR00_evid00081.csv\n",
      "./data/lunar/data/test/data/Filt_S16_GradeA\\xa.s16.00.mhz.1973-07-31HR00_evid00123.csv\n",
      "./data/lunar/data/test/data/Filt_S16_GradeA\\xa.s16.00.mhz.1974-05-19HR00_evid00146.csv\n",
      "./data/lunar/data/test/data/Filt_S16_GradeA\\xa.s16.00.mhz.1974-11-11HR00_evid00160.csv\n",
      "./data/lunar/data/test/data/Filt_S16_GradeA\\xa.s16.00.mhz.1974-12-12HR02_evid00168.csv\n",
      "./data/lunar/data/test/data/Filt_S16_GradeA\\xa.s16.00.mhz.1974-12-15HR00_evid00172.csv\n",
      "./data/lunar/data/test/data/Filt_S16_GradeA\\xa.s16.00.mhz.1974-12-25HR00_evid00174.csv\n",
      "./data/lunar/data/test/data/Filt_S16_GradeA\\xa.s16.00.mhz.1975-02-19HR00_evid00180.csv\n",
      "./data/lunar/data/test/data/Filt_S16_GradeA\\xa.s16.00.mhz.1975-03-26HR00_evid00186.csv\n",
      "./data/lunar/data/test/data/Filt_S16_GradeA\\xa.s16.00.mhz.1977-04-17HR00_evid00249.csv\n",
      "./data/lunar/data/test/data/Filt_S16_GradeA\\xa.s16.00.mhz.1977-06-02HR00_evid00255.csv\n",
      "    chunk  zero_crossing_rate\n",
      "0     NaN                 0.0\n",
      "1     NaN                24.0\n",
      "2     NaN                 0.0\n",
      "3     NaN                 0.0\n",
      "4     NaN                 0.0\n",
      "..    ...                 ...\n",
      "798   NaN                 0.0\n",
      "799   NaN                 0.0\n",
      "800   NaN                 0.0\n",
      "801   NaN                 0.0\n",
      "802   NaN                 0.0\n",
      "\n",
      "[803 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'DataFrame'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15080\\2875635058.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Initialize the Random Forest Classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mclf3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"balanced\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mclf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# Make predictions on the test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alvan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1469\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1470\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1471\u001b[0m                 )\n\u001b[0;32m   1472\u001b[0m             ):\n\u001b[1;32m-> 1473\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\alvan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    364\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alvan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    646\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"estimator\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m                     \u001b[0mcheck_y_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alvan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1297\u001b[0m         raise ValueError(\n\u001b[0;32m   1298\u001b[0m             \u001b[1;34mf\"{estimator_name} requires y to be passed, but the target y is None\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m         )\n\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1301\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m   1302\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alvan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                         )\n\u001b[0;32m   1010\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1014\u001b[0m                 raise ValueError(\n\u001b[0;32m   1015\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m                 ) from complex_warning\n",
      "\u001b[1;32mc:\\Users\\alvan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    741\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 745\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m         \u001b[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m         \u001b[1;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alvan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m   2149\u001b[0m     def __array__(\n\u001b[0;32m   2150\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2151\u001b[0m     ) -> np.ndarray:\n\u001b[0;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2153\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2154\u001b[0m         if (\n\u001b[0;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2156\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# Option 3 - USED\n",
    "print(testing_chunks)\n",
    "# Split the dataset into features (X) and target labels (y)\n",
    "X = testing_chunks.drop(columns=['label'])  # Features\n",
    "y = testing_chunks['label']  # Target labels (1 for quake, 0 for non-quake)\n",
    "\n",
    "\n",
    "X_test = pd.DataFrame(columns=[\"chunk\"]) \n",
    "\n",
    "\n",
    "# Split data into training and testing sets (80% training, 20% testing)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "row = glob.glob(os.path.join(raw_data_dir, '*.csv'))\n",
    "for i in row:\n",
    "    print(i)\n",
    "    X_sample, nope = process_test_file_for_model(i, chunk_size)\n",
    "    X_test = pd.concat([X_test, X_sample], ignore_index=True)\n",
    "    \n",
    "print(X_test)\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "clf3 = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf3.fit(X, y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf3.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{raw_data_dir}/{file_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_testing_df, chuncks_testing_df = process_raw_file_for_model(raw_data_dir, file_name, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chuncks_testing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_testing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = clf2.predict(ready_testing_df)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_of_ones = np.where(predicted_labels == 1)[0]\n",
    "indices_of_ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chunk(chunk):\n",
    "    csv_times = np.array(chunk[\"time_rel(sec)\"].tolist())\n",
    "    csv_velocities = np.array(chunk[\"velocity(m/s)\"].tolist())\n",
    "\n",
    "    # Plot the trace!\n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,3))\n",
    "    ax.plot(csv_times,csv_velocities)\n",
    "    # Make the plot pretty\n",
    "    ax.set_xlim([min(csv_times),max(csv_times)])\n",
    "    ax.set_ylabel('Velocity (m/s)')\n",
    "    ax.set_xlabel('Time (s)')\n",
    "\n",
    "for index in indices_of_ones:\n",
    "    chunk_plot = chuncks_testing_df[\"chunks\"].values[index]\n",
    "    plot_chunk(chunk_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whole Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(raw_data_dir, file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
